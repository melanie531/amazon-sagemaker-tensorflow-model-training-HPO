{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bad673b-74a6-4259-932f-aa4617dda2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "import pathlib\n",
    "public_bucket = \"sagemaker-sample-files\"\n",
    "local_data_dir = \"./data\"\n",
    "path = pathlib.Path(local_data_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download training and testing data from a public S3 bucket\n",
    "def download_from_s3(data_dir=\"/tmp/data\", train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # project root\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    # download objects\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = public_bucket\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3(local_data_dir, True)\n",
    "download_from_s3(local_data_dir, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58789be7-9af1-4e4b-8e24-31ede902ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74519598-b69d-4ffc-8a0b-f600adfbe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model object\n",
    "\n",
    "\n",
    "class SmallConv(Model):\n",
    "    def __init__(self):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation=\"relu\")\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation=\"relu\")\n",
    "        self.d2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "# Decode and preprocess data\n",
    "def convert_to_numpy(data_dir, images_file, labels_file):\n",
    "    \"\"\"Byte string to numpy arrays\"\"\"\n",
    "    with gzip.open(os.path.join(data_dir, images_file), \"rb\") as f:\n",
    "        images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
    "\n",
    "    with gzip.open(os.path.join(data_dir, labels_file), \"rb\") as f:\n",
    "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    return (images, labels)\n",
    "\n",
    "\n",
    "def mnist_to_numpy(data_dir, train):\n",
    "    \"\"\"Load raw MNIST data into numpy array\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): directory of MNIST raw data.\n",
    "            This argument can be accessed via SM_CHANNEL_TRAINING\n",
    "\n",
    "        train (bool): use training data\n",
    "\n",
    "    Returns:\n",
    "        tuple of images and labels as numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    return convert_to_numpy(data_dir, images_file, labels_file)\n",
    "\n",
    "\n",
    "def normalize(x, axis):\n",
    "    eps = np.finfo(float).eps\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    # avoid division by zero\n",
    "    std = np.std(x, axis=axis, keepdims=True) + eps\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "# Training logic\n",
    "\n",
    "\n",
    "def train():\n",
    "    # create data loader from the train / test channels\n",
    "    x_train, y_train = mnist_to_numpy(data_dir=train_data, train=True)\n",
    "    x_test, y_test = mnist_to_numpy(data_dir=test_data, train=False)\n",
    "\n",
    "    x_train, x_test = x_train.astype(np.float32), x_test.astype(np.float32)\n",
    "\n",
    "    # normalize the inputs to mean 0 and std 1\n",
    "    x_train, x_test = normalize(x_train, (1, 2)), normalize(x_test, (1, 2))\n",
    "\n",
    "    # expand channel axis\n",
    "    # tf uses depth minor convention\n",
    "    x_train, x_test = np.expand_dims(x_train, axis=3), np.expand_dims(x_test, axis=3)\n",
    "\n",
    "    # normalize the data to mean 0 and std 1\n",
    "    train_loader = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        .shuffle(len(x_train))\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "    model = SmallConv()\n",
    "    model.compile()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2\n",
    "    )\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=True)\n",
    "            loss = loss_fn(labels, predictions)\n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "        return\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(images, labels):\n",
    "        predictions = model(images, training=False)\n",
    "        t_loss = loss_fn(labels, predictions)\n",
    "        test_loss(t_loss)\n",
    "        test_accuracy(labels, predictions)\n",
    "        return\n",
    "\n",
    "    logger.info(\"Training starts ...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "        for batch, (images, labels) in enumerate(train_loader):\n",
    "            train_step(images, labels)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch + 1}, \"\n",
    "            f\"Loss: {train_loss.result()}, \"\n",
    "            f\"Accuracy: {train_accuracy.result()}, \"\n",
    "        )\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            test_step(images, labels)\n",
    "\n",
    "        # metric for the hyperparameter tunner\n",
    "        logger.info(f\"Test Loss: {test_loss.result()}\")\n",
    "        logger.info(f\"Test Accuracy: {test_accuracy.result()}\")\n",
    "\n",
    "    # Save the model\n",
    "    # A version number is needed for the serving container\n",
    "    # to load the model\n",
    "    version = \"00000000\"\n",
    "    ckpt_dir = os.path.join(model_dir, version)\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    model.save(ckpt_dir)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356f34a6-3244-49af-975b-feded711f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs=1\n",
    "learning_rate=1e-3\n",
    "beta_1=0.9\n",
    "beta_2=0.999\n",
    "\n",
    "# Environment variables given by the training image\n",
    "model_dir=\"./model\"\n",
    "path = pathlib.Path(model_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "train_data = \"./data\"\n",
    "test_data = \"./data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7d4ce1-e64e-47ff-a11d-4e035ec7f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts ...\n",
      "[2022-09-21 01:39:51.928 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:52 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-21 01:39:52.134 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:52 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Epoch 1, Loss: 0.13681690394878387, Accuracy: 0.958816647529602, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 1, Loss: 0.13681690394878387, Accuracy: 0.958816647529602, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06367088109254837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Test Loss: 0.06367088109254837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9787999987602234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Test Accuracy: 0.9787999987602234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/00000000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/00000000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 5.53 s, total: 1min 15s\n",
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1be5f-2193-4a3e-ac43-4fcb457dfdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
